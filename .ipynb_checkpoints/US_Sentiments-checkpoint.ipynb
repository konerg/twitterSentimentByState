{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://datasciencedegree.wisconsin.edu/wp-content/themes/data-gulp/images/logo.svg\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project:  \n",
    "#### Using Twitter feed Validate If People of NV Doing Social Distancing Better than other States such as CA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gkoner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gkoner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gkoner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "ERROR:root:File `'twitter_access_token.py'` not found.\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages and load credentials\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import jsonpickle\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "import re, string, random,pickle,nltk,time\n",
    "\n",
    "### these downloaded files are needed to successfully run the Sentiment analyzer.\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# The following two variables are set to false. \n",
    "# These can be changed to True in order to get new tweets for analysis or training sentiment analyzer \n",
    "\n",
    "download_new_tweets_for_analysis = False\n",
    "download_new_tweets_for_training_sentiment_analyzer = False\n",
    "\n",
    "\n",
    "# If this is set to true a CSV file needed for R will be overwritten when the code is run to the end\n",
    "# This might affect the p-value in R hypothesis test\n",
    "\n",
    "overwrite_final_csv_for_R = False\n",
    "\n",
    "#loading the running the twitter credentials\n",
    "\n",
    "%run \"twitter_access_token.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use tweepy.OAuthHandler to create an authentication using the given key and secret\n",
    "auth = tweepy.OAuthHandler(con_key, con_secret)\n",
    "auth.set_access_token(acc_token, acc_secret)\n",
    "\n",
    "#Connect to the Twitter API using the authentication\n",
    "#api = tweepy.API(auth)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function writes the a dictionary object to a csv file\n",
    "def writeDictCSV(my_dict,csv_name):\n",
    "    import csv\n",
    "    with open(csv_name+'.csv',\"w\",newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerows(my_dict.items()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part of the code gets the ids for the states we want to download the tweets\n",
    "# geostate.name == stateAbbrev condition makes sure we pick correct id for the state\n",
    "# Additional state codes were added for testing purpose\n",
    "\n",
    "states = pd.read_csv(\"state_codes.csv\")\n",
    "\n",
    "#help(pd.read_csv)\n",
    "\n",
    "# if download_new_tweets_for_analysis:\n",
    "    \n",
    "#     for index, row in states.iterrows():\n",
    "       \n",
    "#         geostates = api.geo_search(query=row['Code'], granularity=\"city\") \n",
    "#         for geostate in geostates:\n",
    "#             if geostate.name == row['State']:\n",
    "#                 states.loc[index, 'PlaceId'] = geostate.id\n",
    "\n",
    "\n",
    "    #we write the state ids to a file so that we don't need to execute the geo lookup for state more than once            \n",
    "    #writeDictCSV(statePlaceId,\"State_PlaceId\") \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.to_csv(\"state_codes.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment(object):\n",
    "    \n",
    "    # this class analyzes sentiment for the tweets to indicate if those are for or against the social distancing\n",
    "    # get_classfier() function returns a classifier which can be used analyze the sentiment by calling the method get_sentiment()\n",
    "    # \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "    # this constructor reads 'positive_cleaned_tokens_list.txt' and 'negative_cleaned_tokens_list.txt' from file and \n",
    "    # uses these to train a model to recognize if a tweet is for or against the social distancing\n",
    "    # it also reads 'the stop_words from 'stop_words.txt' file and use those to clean the tweets\n",
    "    \n",
    "\n",
    "        with open ('positive_cleaned_tokens_list.txt', 'rb') as fp:\n",
    "            positive_cleaned_tokens_list = pickle.load(fp)\n",
    "\n",
    "        with open ('negative_cleaned_tokens_list.txt', 'rb') as fp:\n",
    "            negative_cleaned_tokens_list = pickle.load(fp)\n",
    "\n",
    "            \n",
    "        #making sure equal amount positive and negative tweets are taken for training the model\n",
    "        length = min(len(negative_cleaned_tokens_list),len(positive_cleaned_tokens_list))\n",
    "        positive_cleaned_tokens_list = positive_cleaned_tokens_list[:length]\n",
    "        negative_cleaned_tokens_list = negative_cleaned_tokens_list[:length]\n",
    "        \n",
    "        all_pos_words = self.get_all_words(positive_cleaned_tokens_list)\n",
    "        \n",
    "        freq_dist_pos = FreqDist(all_pos_words)\n",
    "\n",
    "        positive_tokens_for_model = self.get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "        negative_tokens_for_model = self.get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "\n",
    "        positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                             for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "        negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                             for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "        dataset = positive_dataset + negative_dataset\n",
    "        random.shuffle(dataset)\n",
    "\n",
    "        # this part was used to test the classifier\n",
    "#         train_data = dataset[:1500]\n",
    "#         test_data = dataset[1500:]\n",
    "\n",
    "        train_data = dataset\n",
    "\n",
    "        self.classifier = NaiveBayesClassifier.train(train_data)\n",
    "    \n",
    "        \n",
    "    def get_classfier(self):\n",
    "        return self.classifier\n",
    "\n",
    "    \n",
    "    def get_sentiment(self,text_tokens):\n",
    "        # this function takes a string as input and returns it's sentiment\n",
    "        # If the text supports social distancing, it returns 1\n",
    "        # If the text is against social distancing, it returns -1\n",
    "        \n",
    "        tweet_tokens = self.lemmatize_tokens(text_tokens)\n",
    "        \n",
    "        sentiment = self.get_classfier().classify(dict([token, True] for token in tweet_tokens))\n",
    "        \n",
    "        #these two prints were used test if the sentiment matched with the tweet\n",
    "        #print(tweet_text)\n",
    "        #print(sentiment)\n",
    "        \n",
    "        return_value = 0\n",
    "        \n",
    "        if sentiment == 'Positive':\n",
    "            return_value = 1\n",
    "        else: \n",
    "            if sentiment == 'Negative':\n",
    "                return_value = -1\n",
    "\n",
    "        return return_value\n",
    "        \n",
    "    \n",
    "\n",
    "    def get_all_words(self,cleaned_tokens_list):\n",
    "        # this function takes the list of strings as input and produces a generator consisting of the strings\n",
    "        for tokens in cleaned_tokens_list:\n",
    "            for token in tokens:\n",
    "                yield token\n",
    "\n",
    "                \n",
    "    def get_tweets_for_model(self,cleaned_tokens_list):\n",
    "        # this function takes the list of strings and returns a generator of dictionary dictionaries \n",
    "        # with keys as the tokens and True as values \n",
    "        for tweet_tokens in cleaned_tokens_list:\n",
    "            yield dict([token, True] for token in tweet_tokens)\n",
    "            \n",
    "    def lemmatize_tokens(self,tweet_tokens):\n",
    "        # This function normalizes the tweet tokens. It takes a list of tokens as input and returns\n",
    "        # normalized tokens\n",
    "        \n",
    "        lemmatize_tokens = []\n",
    "        for token, tag in pos_tag(tweet_tokens):\n",
    "            if tag.startswith(\"NN\"):\n",
    "                pos = 'n'\n",
    "            elif tag.startswith('VB'):\n",
    "                pos = 'v'\n",
    "            else:\n",
    "                pos = 'a'\n",
    "\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            token = lemmatizer.lemmatize(token, pos)\n",
    "            lemmatize_tokens.append(token.lower())\n",
    "\n",
    "        return lemmatize_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllTweets(object):\n",
    "    \n",
    "    # This class is for downloading tweets using REST API.\n",
    "    # It has multiple method for different need\n",
    "    # objects of this class is used in many places to download specific tweets as per our need\n",
    "    # this class also has get_tweet_dataframe() method which json file as input and \n",
    "    # returns a pandas datafrome with specific fields needed for this project\n",
    "    # \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.all_tweet_list = []\n",
    "        self.fileName = None\n",
    "        \n",
    "    def get_tweet_dataframe(self,fileName,sentiment_analysis=True):\n",
    "        # this functions takes a json file as input and returns a pandas dataframe with specific fields for all unique\n",
    "        # tweets in the file. Dataframe fields :'tweet_id','tweet_text','created_at','user_screen_name','place_id',\n",
    "        # 'place_name','language','sentiment'\n",
    "        # It calculates sentiment by calling Sentiment object if argument sentiment_analysis is set to True\n",
    "        # It also prints the row count of dataframe and count of positive and negative tweets and provide their ratio\n",
    "        \n",
    "        import pandas as pd\n",
    "        column_names = ('tweet_id','tweet_text','created_at','user_screen_name','place_id',\n",
    "                        'place_name','language','sentiment')\n",
    "        dict_list = []\n",
    "        \n",
    "        # instantiating Sentiment object. We can create this object once and call get_sentiment() method \n",
    "        # in a loop to get the sentiments. No need to train the classifier each time we need sentiment calculated.\n",
    "        \n",
    "        if sentiment_analysis:\n",
    "            sentimentObject = Sentiment()\n",
    "        \n",
    "        \n",
    "        with open(fileName, 'r') as tweet_file:\n",
    "            for line in tweet_file:\n",
    "                tweet = json.loads(line)\n",
    "                tweetObject = Tweet(tweet)\n",
    "                #discard all the non engligh tweet which were collected in the early stage\n",
    "                if tweetObject.language == 'en' :\n",
    "                    ### calculating sentiment of the tweet using custom Sentiment object\n",
    "                    if sentiment_analysis:\n",
    "                        sentiment = sentimentObject.get_sentiment(tweetObject.text_tokens)\n",
    "                    temp_list = [tweetObject.tweet_id,tweetObject.text, tweetObject.created_at,tweetObject.screen_name,\n",
    "                                tweetObject.place_id,tweetObject.place_name, tweetObject.language, sentiment]\n",
    "                \n",
    "                    dict_list.append(dict(zip(column_names,temp_list)))\n",
    "        \n",
    "        #creating pandas Dataframe and removing duplicates if any  \n",
    "        df = pd.DataFrame(dict_list).drop_duplicates()\n",
    "        \n",
    "        self.print_sentiment_count(df,fileName)       \n",
    "        return df\n",
    "    \n",
    "    def get_unique_tweet_ids(self,fileName):\n",
    "        #this function takes a json file and returns a set containing all unique tweets in the file\n",
    "        tweet_ids = []\n",
    "        line = ''\n",
    "        \n",
    "\n",
    "        try: \n",
    "            with open(fileName, 'r') as tweet_file:\n",
    "                for line in tweet_file:\n",
    "                    tweet = json.loads(line)\n",
    "                    tweet_ids.append(tweet['id'])\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(\"No Prior Tweet File Exists For The State\")\n",
    "        except :\n",
    "            print(line)\n",
    "        #returns only the unique tweet ids\n",
    "        return set(tweet_ids)\n",
    "\n",
    "        \n",
    "    def download_tweet_all_states(self,search_list,maxTweets=5000):\n",
    "        \n",
    "        # this function takes the following inputs\n",
    "        # search list = a list of words that will be used to search twitter\n",
    "        # maxTweets = Maximum number of tweets we want to collect \n",
    "        # it downloads tweets for three states (WI,CA,NV) the new tweets found in the search result and prints the count\n",
    "        # added additional state \"WI\" for testing data\n",
    "        \n",
    "        states = pd.read_csv(\"state_codes.csv\")\n",
    "        \n",
    "        old_df = pd.read_csv(\"tweets.csv\")\n",
    "        unique_tweet_ids = set(old_df['Id'])\n",
    "\n",
    "        for index, row in states.iterrows():\n",
    "            \n",
    "            searchQuery = 'place:' + row['PlaceId']\n",
    "\n",
    "\n",
    "            df = self.download_tweet_for_state(row['Code'],unique_tweet_ids,search_list,searchQuery,5000)\n",
    "            print(f\"{row['Code']} : {len(df.index)}\")\n",
    "            \n",
    "            old_df = old_df.append(df)\n",
    "                  \n",
    "        old_df.to_csv(\"tweets.csv\",index=False)\n",
    "        \n",
    "       \n",
    "    def download_tweet_for_state(self,state, unique_tweet_ids,search_list,searchQuery=None,maxTweets=5000):\n",
    "        # this function takes the following inputs\n",
    "        # state = name of the state the tweet should be downloaded\n",
    "        # search list = a list of words that will be used to search twitter\n",
    "        # maxTweets = Maximum number of tweets we want to collect \n",
    "        # it returns the number of tweets downloaded\n",
    "\n",
    "        #The twitter Search API allows up to 100 tweets per query\n",
    "        \n",
    "        tweetsPerQry = 100\n",
    "        fileName=\"tweets.csv\"\n",
    "        \n",
    "#         state_placeId=pd.read_csv(\"State_PlaceId.csv\", header=None)\n",
    "#         state_placeId.columns = ['State', 'PlaceId']\n",
    "#         if state == \"NV\":\n",
    "#             self.fileName = \"NV_Tweets_socialdistancing.json\"            \n",
    "#             searchQuery = 'place:' + \\\n",
    "#                     state_placeId.iloc[4]['PlaceId']\n",
    "#         if state == \"CA\":\n",
    "#             self.fileName = \"CA_Tweets_socialdistancing.json\"            \n",
    "#             searchQuery = 'place:' + \\\n",
    "#                     state_placeId.iloc[0]['PlaceId']\n",
    "        \n",
    "      \n",
    "        tweetCount = 0\n",
    "        \n",
    "      \n",
    "\n",
    "        dtypes = np.dtype([\n",
    "          ('Id', str),\n",
    "          ('Text', str),\n",
    "          ('Created_At',str),\n",
    "          ('State',str)\n",
    "          ])\n",
    "        data = np.empty(0, dtype=dtypes)\n",
    "\n",
    "        tweet_df = pd.DataFrame(data)\n",
    "    \n",
    "        for idx, item in enumerate(search_list):\n",
    "            if idx+1 == len(search_list):\n",
    "                searchQuery = searchQuery + ' \\\"' + item + '\\\"'\n",
    "            else:\n",
    "                searchQuery = searchQuery + ' \\\"' + item + '\\\"'+' OR'\n",
    "\n",
    "        #Open a text file to save the tweets to\n",
    "    \n",
    "        #filemode 'w' is used to overwrite the existing file\n",
    "        \n",
    "            #Tell the Cursor method that we want to use the Search API (api.search)\n",
    "            #Also tell Cursor our query, and the maximum number of tweets to return\n",
    "            \n",
    "        for tweet in tweepy.Cursor(api.search,q=searchQuery,tweet_mode=\"extended\").items(maxTweets) :         \n",
    "\n",
    "            #Verify the tweet has place info before writing (It should, if it got past our place filter)\n",
    "            if tweet.place is not None:\n",
    "                #verify if the tweet is not already in the file from a previous download\n",
    "                if tweet.id not in unique_tweet_ids:\n",
    "                    #only need the tweet with English language\n",
    "                    if tweet.lang == 'en':\n",
    "\n",
    "                        tweet_df.loc[tweetCount] = [tweet.id,tweet.full_text,tweet.created_at,state]\n",
    "                        #Write the JSON format to the text file, and add one to the number of tweets we've collected\n",
    "#                             f.write(jsonpickle.encode(tweet.full_text, unpicklable=False))\n",
    "#                             f.write(jsonpickle.encode(tweet.created_at, unpicklable=False) +\\n)\n",
    "                        tweetCount += 1\n",
    "\n",
    "\n",
    "            # Returns how many tweets we have collected\n",
    "        return tweet_df\n",
    "\n",
    "    def download_tweets(self, fileName,search_list,maxTweets=10000):\n",
    "        # this function takes the following inputs and download tweets to the file\n",
    "        # state = name of the state the tweet should be downloaded\n",
    "        # fileName = name of the file to save the downloaded tweets\n",
    "        # search list = a list of words that will be used to search twitter\n",
    "        # maxTweets = Maximum number of tweets we want to collect \n",
    "        # it returns the number of tweets downloaded\n",
    "\n",
    "        #The twitter Search API allows up to 100 tweets per query\n",
    "        tweetsPerQry = 100\n",
    "        tweetCount = 0\n",
    "        \n",
    "        searchQuery = ''\n",
    "        for idx, item in enumerate(search_list):\n",
    "            if idx+1 == len(search_list):\n",
    "                searchQuery = searchQuery + ' \\\"' + item + '\\\"'\n",
    "            else:\n",
    "                searchQuery = searchQuery + ' \\\"' + item + '\\\"'+' OR'\n",
    "                \n",
    "        #print(searchQuery)\n",
    "        #Open a text file to save the tweets to\n",
    "        try: \n",
    "            with open(fileName, 'w') as f:\n",
    "            #filemode 'w' is used to overwrite the existing file\n",
    "\n",
    "                #Tell the Cursor method that we want to use the Search API (api.search)\n",
    "                #Also tell Cursor our query, and the maximum number of tweets to return\n",
    "                for tweet in tweepy.Cursor(api.search,q=searchQuery,tweet_mode=\"extended\").items(maxTweets) :         \n",
    "\n",
    "                    #only need the tweet with English language\n",
    "                    if tweet.lang == 'en':\n",
    "                        #Write the JSON format to the text file, and add one to the number of tweets we've collected\n",
    "                        f.write(jsonpickle.encode(tweet._json, unpicklable=False) + '\\n')\n",
    "                        tweetCount += 1\n",
    "\n",
    "\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(\"Wrong file or file path\")\n",
    "        except :\n",
    "            print(\"Error downloading\")\n",
    "        #Returns how many tweets we have collected\n",
    "        return tweetCount\n",
    "\n",
    "    def print_sentiment_count(self, df,fileName=None):\n",
    "        # this function takes a dataframe and a fileName as \n",
    "        # as inout and prints the row count and count of positive and negative sentiment.\n",
    "        import pandas as pd\n",
    "        positive = len(df[df.sentiment == 1])\n",
    "        negative = len(df[df.sentiment == -1])\n",
    "        print(f\"File Name: {fileName}\")\n",
    "        print(f\"Total tweets: {len(df)}\")\n",
    "        print(f\"Positive: {positive}\") \n",
    "        print(f\"Negative: {negative}\")\n",
    "        print(f\"Ratio: {round(positive/negative,2)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_token_list(fileName,listName):\n",
    "    # this function converts the downloaded positive or negative tweets to a list of tokens and save those in files\n",
    "    # list of tokens files are used by the sentiment analyzer \n",
    "    import json\n",
    "    import pickle\n",
    "    \n",
    "    temp_list = []\n",
    "    \n",
    "    hashtags = ['stayathomesaveslife','keepseniorssafe','socialdistancingworks',\n",
    "                      'stayathomeandstaysafe','openamericanow']\n",
    "   \n",
    "    with open(fileName, 'r' ) as fi:\n",
    "        for line in fi:\n",
    "\n",
    "            tweet = json.loads(line)\n",
    "            tweetObject = Tweet(tweet)\n",
    "            # ignoring all the non engligh tweets \n",
    "\n",
    "            #print(tweetObject.text_tokens)\n",
    "\n",
    "            if tweetObject.language == 'en' :\n",
    "                #excluding Retweets for training the sentiment analyzer \n",
    "                if tweetObject.text.find('rt') != 0:\n",
    "\n",
    "                    tweet_tokens = tweetObject.text_tokens\n",
    "                    \n",
    "                     \n",
    "\n",
    "                    #removing the search word from the sentiment analyzer list as those may have higher \n",
    "                    # presence than other words \n",
    "\n",
    "                    tweet_tokens = [ii for ii in tweet_tokens if ii not in hashtags]\n",
    "\n",
    "                    #print(tweet_tokens)\n",
    "\n",
    "                    lemmatize_tokens = []\n",
    "\n",
    "                    for token, tag in pos_tag(tweet_tokens):\n",
    "\n",
    "                        if tag.startswith(\"NN\"):\n",
    "                            pos = 'n'\n",
    "                        elif tag.startswith('VB'):\n",
    "                            pos = 'v'\n",
    "                        else:\n",
    "                            pos = 'a'\n",
    "\n",
    "                        #calling lemmatizer in NLTK package for normalizing the tweet text tokens\n",
    "\n",
    "                        lemmatizer = WordNetLemmatizer()\n",
    "                        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "\n",
    "                        lemmatize_tokens.append(token.lower())\n",
    "\n",
    "                    temp_list.append(lemmatize_tokens)\n",
    "\n",
    "\n",
    "    with open (listName + '.txt', 'wb') as fp:\n",
    "        pickle.dump(temp_list, fp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_the_token_list(fileName,text):\n",
    "    # this function can be used to add to the positive or negative tweet database for the sentiment classifier\n",
    "    import pickle\n",
    "    try:\n",
    "        toke_list = []\n",
    "        with open (fileName, 'rb') as fp:\n",
    "            tokens_list = pickle.load(fp)\n",
    "            \n",
    "        tokens_list.append(text.split())\n",
    "        \n",
    "        with open (fileName , 'wb') as fp:\n",
    "            pickle.dump(tokens_list, fp)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error reading file / file does not exist\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_token_list(fileName):\n",
    "# this function takes a filename as input and prints the list in the file\n",
    "# \n",
    "    import pickle\n",
    "    try:\n",
    "        with open (fileName, 'rb') as fp:\n",
    "            tokens_list = pickle.load(fp)\n",
    "            print(tokens_list)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error reading file / file does not exist\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section was used to visually inspect the cleaned token list\n",
    "# print_token_list(\"negative_cleaned_tokens_list.txt\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section was used to add some sentences from a file to the sentiment analysis dataset\n",
    "# with open('manual_positive_texts.txt',encoding='UTF-8') as file:\n",
    "#     try: \n",
    "#         for line in file: \n",
    "#             print(line)\n",
    "#             add_to_the_token_list(\"positive_cleaned_tokens_list.txt\",text)\n",
    "#     except: \n",
    "#         print(\"error\")\n",
    "\n",
    "\n",
    "# with open('manual_positive_texts.txt',encoding='UTF-8') as file:\n",
    "#     try: \n",
    "#         for line in file: \n",
    "#             print(line)\n",
    "#             add_to_the_token_list(\"negative_cleaned_tokens_list.txt\",text)\n",
    "#     except: \n",
    "#         print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### change the global variable value to True at top to execute this code\n",
    "\n",
    "if download_new_tweets_for_training_sentiment_analyzer:\n",
    "#searching twitter for positive tweets that supports social distancing\n",
    "    search_list = [             \n",
    "                    'stayathomesaveslife','keepSeniorsSafe','socialdistancingworks','stayathomeandstaysafe'\n",
    "                ]\n",
    "\n",
    "\n",
    "    AllTweets().download_tweets(\"positive_tweets.json\",search_list,5000)\n",
    "    save_token_list(\"positive_tweets.json\",'positive_cleaned_tokens_list')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### change the global variable value to True at top to execute this code\n",
    "\n",
    "if download_new_tweets_for_training_sentiment_analyzer:\n",
    "#searching twitter for negative tweets do not support social distancing\n",
    "    search_list = [             \n",
    "                    \"#openamericanow\"\n",
    "                ]\n",
    "\n",
    "    AllTweets().download_tweets(\"negative_tweets.json\",search_list,5000)\n",
    "    save_token_list(\"negative_tweets.json\",'negative_cleaned_tokens_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section we perform the main twitter search for tweets containing words about social distancing\n",
    "# This was executed everyday to collect any new tweets\n",
    "# only the new tweets were appended to the download as the download_tweet_all_states function prevents the duplicate tweets\n",
    "\n",
    "# change the global variable at the top to True to execute to get new tweets for analysis\n",
    "\n",
    "if download_new_tweets_for_analysis:\n",
    "    search_list = [             \n",
    "                    \"socialdist\", \"social distan\", \"shelter in\", \n",
    "                    \"lockdown\", \"shelterin\", \"quarant\",\"from home\",\"fromhome\",\n",
    "                     \"ing from home\",\"ing remote\"\n",
    "                 ]\n",
    "    \n",
    "    AllTweets().download_tweet_all_states(search_list) \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"tweets.csv\")\n",
    "\n",
    "tweets['Text'] = tweets['Text'].apply(remove_stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentObject = Sentiment()\n",
    "tweets['Sentiment'] = tweets['Text'].apply(sentimentObject.get_sentiment)\n",
    "tweets['Created_At'] = tweets['Created_At'].str.slice(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv(\"tweets_R.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #this function takes a text as input and returns the string back after removing punctuations, url, stop words etc\n",
    "\n",
    "\n",
    "    try: \n",
    "\n",
    "        text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "            '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', text)\n",
    "        text = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", text)\n",
    "\n",
    "\n",
    "    except:\n",
    "        print(text)\n",
    "\n",
    "    return depunctify(text)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "# this function takes a text as input and returns a list containing tokens of the texts after \n",
    "# removing the stop words\n",
    "# stop words are read from a file\n",
    "    text = clean_text(text)\n",
    "    tokens = text.lower().split()\n",
    "    with open (\"stop_words.txt\", 'rb') as fp:\n",
    "        stop_words = pickle.load(fp)\n",
    "    #removing the stopwords from the token list      \n",
    "    tokens = [ii for ii in tokens if ii not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def depunctify(st):\n",
    "        ### takes a string and returns the string after removing the punctuations\n",
    "    import string\n",
    "    st = st.replace(\"-\", \"\")\n",
    "    st = st.replace(\"\\n\", \" \")\n",
    "    st = st.replace(\"\\t\", \" \")\n",
    "    for ii in string.punctuation + \"‘’”“\":\n",
    "        st = st.replace(ii,'')\n",
    "    return st "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
